{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume messages from an Aiven for Apache Kafka®️ partitioned topic\n",
    "\n",
    "In the [previous notebook](4-consume-partition-0.ipynb), we explored consuming topics in separate partitions.\n",
    "\n",
    "![Consume messages from an Apache Kafka Topic](../img/5%20-%20multiple%20kafka%20consumer%20groups%20python.png)\n",
    "\n",
    "You may have noticed in our `conf` that we create a `group.id` value. Up until now we've only had one group but we can have multiple groups that consume our messages differently.\n",
    "\n",
    "Our stores are doing well and our billing department wants to be able to track our **Overall Orders**. We can do this with no disruption to our Aiven for Apache Kafka®️ configuration.\n",
    "\n",
    "In this notebook we'll create two groups. One will be for our stores to manage their orders only, while the second one will consume messages from both partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from confluent_kafka import (\n",
    "    DeserializingConsumer,\n",
    "    KafkaError,\n",
    "    KafkaException,\n",
    "    TopicPartition,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "KAFKA_SERVICE_URI = os.getenv(\"KAFKA_SERVICE_URI\")\n",
    "topic = 'pizzaPartitioned'\n",
    "\n",
    "def json_serializer(msg, s_obj):\n",
    "    return json.loads(msg.decode('ascii'))\n",
    "\n",
    "base_conf = {\n",
    "    'bootstrap.servers': KAFKA_SERVICE_URI,\n",
    "    'client.id': 'myclient',\n",
    "    'group.id': 'base',\n",
    "    'security.protocol': 'SSL',\n",
    "    'ssl.ca.location': '../sslcerts/ca.pem',\n",
    "    'ssl.certificate.location': '../sslcerts/service.cert',\n",
    "    'ssl.key.location': '../sslcerts/service.key', \n",
    "    'value.deserializer': json_serializer,\n",
    "    'key.deserializer': json_serializer\n",
    "    }\n",
    "\n",
    "orders_conf = {**base_conf, 'group.id': 'orders'}\n",
    "\n",
    "# Create a and orders consumer for each store\n",
    "orders_consumer_store_0 = DeserializingConsumer(orders_conf)\n",
    "orders_consumer_store_0.assign([TopicPartition(topic, 0)])\n",
    "\n",
    "orders_consumer_store_1 = DeserializingConsumer(orders_conf)\n",
    "orders_consumer_store_1.assign([TopicPartition(topic, 1)])\n",
    "\n",
    "# Create a centralized billing consumer\n",
    "billing_conf = {**base_conf, 'group.id': 'billing'}\n",
    "billing_consumer = DeserializingConsumer(billing_conf)\n",
    "billing_consumer.assign([TopicPartition(topic, 0), TopicPartition(topic, 1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume messages to the `pizzaPartitioned` topic\n",
    "\n",
    "To show how that the partitions have access to different one or more groups we'll consume our groups using this custom function `display_orders`.\n",
    "\n",
    "This will tell you the Name of the partition and the message that we're used to.\n",
    "\n",
    "We've color-coded the consumers so you'll see the _Northern_Store_ Consumer in Red, the _Southern Store_ in green, and the _Billing_ in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def display_orders(consumer:DeserializingConsumer, label:str, style:str = \"bold\", count:int = 1):\n",
    "    \"\"\"Display the orders from a consumer.\"\"\"\n",
    "    finished = False\n",
    "    local_count = 0\n",
    "    while not finished:\n",
    "        if (msg:=consumer.poll(timeout=1.0)) is None:\n",
    "            continue\n",
    "        elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            console.print(f\"Checking Partition {label} \\n\" + \"-\"*10, style=style)\n",
    "            console.print(f\"[{style}]{msg.partition()}:{msg.offset()}: {msg.key()}:{msg.value()}[/{style}]\\n\\n\")\n",
    "            local_count += 1\n",
    "            finished = local_count == count\n",
    "\n",
    "console.print(\"Run the last block from `Produce-Partitioned-Topics.ipynb`.\")\n",
    "display_orders(orders_consumer_store_0, \"Northern Store\", \"bold red\")\n",
    "display_orders(orders_consumer_store_1, \"Southern Store\", \"bold green\")\n",
    "display_orders(billing_consumer, \"Billing\", \"bold blue\", count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Work\n",
    "\n",
    "In this notebook:\n",
    "  - We configured our three consumers to consume different topics.\n",
    "\n",
    "This is one way of limiting how we consume particular topics. In our penultimate notebook, we'll take a look at a using Aiven for Apache Flink® to filter messages based on a query.\n",
    "\n",
    "Select the [next notebook](6-transform-with-apache-flink.ipynb) or click the button below to continue.\n",
    "\n",
    "[![Button to Part 5](https://img.shields.io/badge/6-Transformations%20Using%20Aiven%20for%20Apache%20Flink-a03586?style=for-the-badge&labelColor=ec6147)](6-transform-with-apache-flink.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
